{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wendycao411/Pedestrian_Tracking/blob/main/Pedestrian_Tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18a5b234",
      "metadata": {
        "id": "18a5b234"
      },
      "source": [
        "# Pedestrian Tracking\n",
        "\n",
        "created by wendycao 2025/1\n",
        "\n",
        "best if run with T4 GPU! it'll be much faster. to change GPU, go to the runtime tab and select \"change runtime type\"."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing dependencies"
      ],
      "metadata": {
        "id": "02QtKK0oQb0O"
      },
      "id": "02QtKK0oQb0O"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install opencv-python\n",
        "%pip install pandas\n",
        "%pip install numpy\n",
        "%pip install ultralytics\n",
        "%pip install roboflow\n",
        "%pip install matplotlib\n",
        "%pip install os-sys\n",
        "%pip install roboflow\n",
        "%pip install inference_sdk\n",
        "%pip install easydict\n",
        "\n",
        "%pip install lap"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35m6MEAA0dle",
        "outputId": "c2f25f26-e141-40a5-e7f2-94c9d173c0c5"
      },
      "id": "35m6MEAA0dle",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.75)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.11/dist-packages (1.1.54)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.1.31)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.23.5)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (10.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.56.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting os-sys\n",
            "  Using cached os_sys-2.1.4-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting pygubu (from os-sys)\n",
            "  Using cached pygubu-0.36.3-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from os-sys) (2025.1)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.11/dist-packages (from os-sys) (0.5.3)\n",
            "Collecting progress (from os-sys)\n",
            "  Using cached progress-1.6.tar.gz (7.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from os-sys) (4.67.1)\n",
            "Collecting progressbar (from os-sys)\n",
            "  Using cached progressbar-2.5.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from os-sys) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from os-sys) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from os-sys) (1.17.0)\n",
            "Collecting jupyter (from os-sys)\n",
            "  Using cached jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from os-sys) (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from os-sys) (4.13.3)\n",
            "Collecting Eel (from os-sys)\n",
            "  Using cached eel-0.18.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting extract-zip (from os-sys)\n",
            "  Using cached extract_zip-1.0.0-py3-none-any.whl.metadata (403 bytes)\n",
            "INFO: pip is looking at multiple versions of os-sys to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting os-sys\n",
            "  Using cached os_sys-2.1.3-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Using cached os_sys-2.1.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Using cached os_sys-2.1.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Using cached os_sys-2.1.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Using cached os_sys-2.0.9-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Using cached os_sys-2.0.8-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Using cached os_sys-2.0.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "INFO: pip is still looking at multiple versions of os-sys to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached os_sys-2.0.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "  Using cached os_sys-2.0.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "  Using cached os_sys-2.0.4-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting webview (from os-sys)\n",
            "  Using cached webview-0.1.5.tar.gz (18 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.11/dist-packages (1.1.54)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.1.31)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.23.5)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (10.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.56.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "Requirement already satisfied: inference_sdk in /usr/local/lib/python3.11/dist-packages (0.37.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.0 in /usr/local/lib/python3.11/dist-packages (from inference_sdk) (2.32.3)\n",
            "Requirement already satisfied: dataclasses-json~=0.6.0 in /usr/local/lib/python3.11/dist-packages (from inference_sdk) (0.6.7)\n",
            "Requirement already satisfied: opencv-python<=4.10.0.84,>=4.8.1.78 in /usr/local/lib/python3.11/dist-packages (from inference_sdk) (4.10.0.84)\n",
            "Requirement already satisfied: pillow<11.0,>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from inference_sdk) (10.4.0)\n",
            "Requirement already satisfied: supervision<=0.30.0,>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from inference_sdk) (0.25.1)\n",
            "Requirement already satisfied: numpy<=1.26.4 in /usr/local/lib/python3.11/dist-packages (from inference_sdk) (1.23.5)\n",
            "Requirement already satisfied: aiohttp<=3.10.11,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from inference_sdk) (3.10.11)\n",
            "Requirement already satisfied: backoff~=2.2.0 in /usr/local/lib/python3.11/dist-packages (from inference_sdk) (2.2.1)\n",
            "Requirement already satisfied: py-cpuinfo~=9.0.0 in /usr/local/lib/python3.11/dist-packages (from inference_sdk) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference_sdk) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference_sdk) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference_sdk) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference_sdk) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference_sdk) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference_sdk) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json~=0.6.0->inference_sdk) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json~=0.6.0->inference_sdk) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.0->inference_sdk) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.0->inference_sdk) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.0->inference_sdk) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.0->inference_sdk) (2025.1.31)\n",
            "Requirement already satisfied: contourpy>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from supervision<=0.30.0,>=0.25.1->inference_sdk) (1.3.1)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from supervision<=0.30.0,>=0.25.1->inference_sdk) (0.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from supervision<=0.30.0,>=0.25.1->inference_sdk) (3.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from supervision<=0.30.0,>=0.25.1->inference_sdk) (6.0.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from supervision<=0.30.0,>=0.25.1->inference_sdk) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.11/dist-packages (from supervision<=0.30.0,>=0.25.1->inference_sdk) (4.67.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json~=0.6.0->inference_sdk) (24.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision<=0.30.0,>=0.25.1->inference_sdk) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision<=0.30.0,>=0.25.1->inference_sdk) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision<=0.30.0,>=0.25.1->inference_sdk) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision<=0.30.0,>=0.25.1->inference_sdk) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision<=0.30.0,>=0.25.1->inference_sdk) (2.8.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json~=0.6.0->inference_sdk) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json~=0.6.0->inference_sdk) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<=3.10.11,>=3.9.0->inference_sdk) (0.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision<=0.30.0,>=0.25.1->inference_sdk) (1.17.0)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.11/dist-packages (1.13)\n",
            "Requirement already satisfied: lap in /usr/local/lib/python3.11/dist-packages (0.5.12)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from lap) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip uninstall numpy -y\n",
        "%pip install \"numpy<1.24\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "cYGWl4no1h-F",
        "outputId": "1794941f-7646-4bac-eab4-7562c9e0f1e6"
      },
      "id": "cYGWl4no1h-F",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.23.5\n",
            "Uninstalling numpy-1.23.5:\n",
            "  Successfully uninstalled numpy-1.23.5\n",
            "Collecting numpy<1.24\n",
            "  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.4 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.88 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.20.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.36.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.0.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "langchain 0.3.18 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "numba 0.61.0 requires numpy<2.2,>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.8 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "dd1284b3ca684a67a3c0af8b95534322"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "27d65368",
      "metadata": {
        "id": "27d65368"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from PIL import Image, ImageDraw\n",
        "from inference_sdk import InferenceHTTPClient\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.utils.plotting import Annotator, colors\n",
        "\n",
        "from roboflow import Roboflow\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "import torch\n",
        "\n",
        "# Define the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(if it asks to restart the runtime, click restart session and then re-run the cell importing modules)"
      ],
      "metadata": {
        "id": "avPzsT8MTBDf"
      },
      "id": "avPzsT8MTBDf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download Videos from Google Drive (requires the download link)"
      ],
      "metadata": {
        "id": "TDQ_5HI0QjMW"
      },
      "id": "TDQ_5HI0QjMW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*How to generate the download link:*\n",
        "\n",
        "\n",
        "1.   Get the \"Share Link\" for the video (make sure anyone can view the video!) - e.g. https://drive.google.com/file/d/1CNjE7CWTuyqgm-zTRGk5qIxSDG8A7P7q/view?usp=drive_link\n",
        "2.   Get the ID for the video, which is in between the /d/ and /view - e.g. 1CNjE7CWTuyqgm-zTRGk5qIxSDG8A7P7q\n",
        "3.   Paste the ID in this link: https://drive.google.com/uc?id=(ID HERE)&export=download -- e.g. https://drive.google.com/uc?id=1CNjE7CWTuyqgm-zTRGk5qIxSDG8A7P7q&export=download\n",
        "4.   Done!\n",
        "\n"
      ],
      "metadata": {
        "id": "ILKgHh_bQnu9"
      },
      "id": "ILKgHh_bQnu9"
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=18nVdSz9Nku_b0jWtIFkY5F99pB5ZeOzm&export=download\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VO2DHGF194Z",
        "outputId": "c6b6e308-27db-4556-9434-ae930e286450"
      },
      "id": "5VO2DHGF194Z",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=18nVdSz9Nku_b0jWtIFkY5F99pB5ZeOzm\n",
            "From (redirected): https://drive.google.com/uc?id=18nVdSz9Nku_b0jWtIFkY5F99pB5ZeOzm&confirm=t&uuid=bf15ed98-06a0-40df-8cc0-b28054cfaff4\n",
            "To: /content/2023-06-04.mp4\n",
            "100% 2.99G/2.99G [00:39<00:00, 76.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OR Download from DropBox (make sure video is cropped)"
      ],
      "metadata": {
        "id": "vmfzK1H2VkAc"
      },
      "id": "vmfzK1H2VkAc"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/scl/fi/ajuar2kemb1gxb6cedtd5/2022-06-13-12-00.mp4?rlkey=hncu700ftarsu9msw0ryuklbi&st=a1jnu4t6&dl=0"
      ],
      "metadata": {
        "id": "edY5KiRhVjL-"
      },
      "id": "edY5KiRhVjL-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing"
      ],
      "metadata": {
        "id": "Fr9BytS2Qsey"
      },
      "id": "Fr9BytS2Qsey"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "d8618912",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8618912",
        "outputId": "4917e6c3-ea91-4c95-97de-486469bcf40f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-04\n"
          ]
        }
      ],
      "source": [
        "video_path = \"/content/2023-06-04.mp4\"\n",
        "\n",
        "file_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "\n",
        "print(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "f4fdd446",
      "metadata": {
        "id": "f4fdd446",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d54d56f7-6f8f-4452-85f0-383a738e5de8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved to /content/2023-06-04_timelapsed5.mp4\n"
          ]
        }
      ],
      "source": [
        "# video preprocessing\n",
        "\n",
        "# option 1 Adjust the frame rate to 5 FPS, and the total duration remains unchanged\n",
        "#ffmpeg -i \"/content/2022-07-31.mp4\" -r 5 \"/content/2022-07-31_5fps.mp4\"\n",
        "\n",
        "#USE ON LARGER VIDEOS\n",
        "# option 2 function: subsample/time lapse video\n",
        "def subsample_video(input_path, output_path, time_lapse_interval):\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Couldn't open video file.\")\n",
        "        return\n",
        "\n",
        "    frame_count = 0\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    codec = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(output_path, codec, fps, (width, height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Collect one frame out of every xx frames\n",
        "        if frame_count % time_lapse_interval == 0:\n",
        "            out.write(frame)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    #cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    time_lapse_interval = 5 # change here\n",
        "    input_file = video_path\n",
        "    output_file = f\"/content/{file_name}_timelapsed{time_lapse_interval}.mp4\"  # Replace with desired output video file path\n",
        "    subsample_video(input_file, output_file, time_lapse_interval)\n",
        "    print(f\"saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "c5b8b88c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5b8b88c",
        "outputId": "18f0d8fa-5a60-49f7-c051-26ef2c600fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/2023-06-04_timelapsed5.mp4\n",
            "2023-06-04_timelapsed5\n"
          ]
        }
      ],
      "source": [
        "video_path = \"/content/2023-06-04_timelapsed5.mp4\"\n",
        "file_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "\n",
        "print(video_path)\n",
        "print(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "ddfbd21e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddfbd21e",
        "outputId": "a62e49e1-6fe7-4fcd-cfdb-8b24d2d07668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of frames in the video：6939\n",
            "Video frame rate：30.0\n"
          ]
        }
      ],
      "source": [
        "# number of frames in the video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get the file name\n",
        "file_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "\n",
        "# Get the total number of frames of the video\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "print(f\"The total number of frames in the video：{total_frames}\\nVideo frame rate：{original_fps}\")\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download the Model!\n",
        "\n",
        "can also download elsewhere and upload to get other models and test if they are better"
      ],
      "metadata": {
        "id": "kxfBP3NPpnCr"
      },
      "id": "kxfBP3NPpnCr"
    },
    {
      "cell_type": "code",
      "source": [
        "#!gdown https://drive.google.com/uc?id=19ZYplT-m0SaXrJXks9DhO7wdIUMpK2_F&export=download\n",
        "\n",
        "#used by Emma, not tested if more accurate yet but is slower\n",
        "#!gdown https://drive.google.com/uc?id=1WFUfBKAB2_U96PwHiI08seJzKk3yeGLg&export=download\n",
        "\n",
        "#another model to test with (the most recent and accurate?? one)\n",
        "!gdown https://drive.google.com/uc?id=1ybVH-ZreQxhXzJ_MTLzMQKPOGcHpFI6S&export=download"
      ],
      "metadata": {
        "id": "QHo6yYoJpzwJ",
        "outputId": "4b571223-a750-44b1-ca42-41e2e27f3253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QHo6yYoJpzwJ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1ybVH-ZreQxhXzJ_MTLzMQKPOGcHpFI6S\n",
            "From (redirected): https://drive.google.com/uc?id=1ybVH-ZreQxhXzJ_MTLzMQKPOGcHpFI6S&confirm=t&uuid=0cf5efae-ded6-4ae5-af5b-c1eab3dbc3f4\n",
            "To: /content/yolo11x-seg.pt\n",
            "100% 125M/125M [00:01<00:00, 80.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To implement the DeepSORT tracking, downloading the required files (only needs to be done once at the beginnning of running the code)"
      ],
      "metadata": {
        "id": "FTjGDzSlQ6pB"
      },
      "id": "FTjGDzSlQ6pB"
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"https://drive.google.com/uc?id=11ZSZcG-bcbueXZC3rN08CM0qqX3eiHxf&confirm=t\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Vg6RPo_2fs-",
        "outputId": "ed94b752-b8fb-4f99-ecc6-be9abb4e631a"
      },
      "id": "2Vg6RPo_2fs-",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11ZSZcG-bcbueXZC3rN08CM0qqX3eiHxf&confirm=t\n",
            "To: /content/deep_sort_pytorch.zip\n",
            "100% 43.1M/43.1M [00:00<00:00, 81.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 'deep_sort_pytorch.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P3lSmN6U2h66",
        "outputId": "ab9b3449-8570-4adc-f71d-8fc08854093a"
      },
      "id": "P3lSmN6U2h66",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  deep_sort_pytorch.zip\n",
            "  inflating: deep_sort_pytorch/.gitignore  \n",
            "   creating: deep_sort_pytorch/configs/\n",
            "  inflating: deep_sort_pytorch/configs/deep_sort.yaml  \n",
            "   creating: deep_sort_pytorch/deep_sort/\n",
            "  inflating: deep_sort_pytorch/deep_sort/__init__.py  \n",
            "   creating: deep_sort_pytorch/deep_sort/__pycache__/\n",
            "  inflating: deep_sort_pytorch/deep_sort/__pycache__/__init__.cpython-310.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/__pycache__/__init__.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/__pycache__/deep_sort.cpython-310.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/__pycache__/deep_sort.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/__pycache__/deep_sort.cpython-38.pyc  \n",
            "   creating: deep_sort_pytorch/deep_sort/deep/\n",
            " extracting: deep_sort_pytorch/deep_sort/deep/__init__.py  \n",
            "   creating: deep_sort_pytorch/deep_sort/deep/__pycache__/\n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/__pycache__/__init__.cpython-310.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/__pycache__/__init__.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/__pycache__/feature_extractor.cpython-310.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/__pycache__/feature_extractor.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/__pycache__/feature_extractor.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/__pycache__/model.cpython-310.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/__pycache__/model.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/__pycache__/model.cpython-38.pyc  \n",
            "   creating: deep_sort_pytorch/deep_sort/deep/checkpoint/\n",
            " extracting: deep_sort_pytorch/deep_sort/deep/checkpoint/.gitkeep  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/checkpoint/ckpt.t7  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/evaluate.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/feature_extractor.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/model.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/original_model.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/test.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/train.jpg  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep/train.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/deep_sort.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/README.md  \n",
            "   creating: deep_sort_pytorch/deep_sort/sort - Copy/\n",
            " extracting: deep_sort_pytorch/deep_sort/sort - Copy/__init__.py  \n",
            "   creating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/\n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/__init__.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/detection.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/detection.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/iou_matching.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/iou_matching.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/kalman_filter.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/kalman_filter.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/linear_assignment.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/linear_assignment.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/nn_matching.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/nn_matching.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/track.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/track.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/tracker.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/__pycache__/tracker.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/iou_matching.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/kalman_filter.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/linear_assignment.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/nn_matching.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort - Copy/preprocessing.py  \n",
            "   creating: deep_sort_pytorch/deep_sort/sort/\n",
            " extracting: deep_sort_pytorch/deep_sort/sort/__init__.py  \n",
            "   creating: deep_sort_pytorch/deep_sort/sort/__pycache__/\n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/__init__.cpython-310.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/__init__.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/detection.cpython-310.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/detection.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/detection.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/iou_matching.cpython-310.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/iou_matching.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/iou_matching.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/kalman_filter.cpython-310.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/kalman_filter.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/kalman_filter.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/linear_assignment.cpython-310.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/linear_assignment.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/linear_assignment.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/nn_matching.cpython-310.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/nn_matching.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/nn_matching.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/track.cpython-310.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/track.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/track.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/tracker.cpython-310.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/tracker.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/__pycache__/tracker.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/detection.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/iou_matching.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/kalman_filter.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/linear_assignment.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/nn_matching.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/preprocessing.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/track.py  \n",
            "  inflating: deep_sort_pytorch/deep_sort/sort/tracker.py  \n",
            "  inflating: deep_sort_pytorch/LICENSE  \n",
            "  inflating: deep_sort_pytorch/README.md  \n",
            "   creating: deep_sort_pytorch/utils/\n",
            " extracting: deep_sort_pytorch/utils/__init__.py  \n",
            "   creating: deep_sort_pytorch/utils/__pycache__/\n",
            "  inflating: deep_sort_pytorch/utils/__pycache__/__init__.cpython-310.pyc  \n",
            "  inflating: deep_sort_pytorch/utils/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/utils/__pycache__/__init__.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/utils/__pycache__/parser.cpython-310.pyc  \n",
            "  inflating: deep_sort_pytorch/utils/__pycache__/parser.cpython-37.pyc  \n",
            "  inflating: deep_sort_pytorch/utils/__pycache__/parser.cpython-38.pyc  \n",
            "  inflating: deep_sort_pytorch/utils/asserts.py  \n",
            "  inflating: deep_sort_pytorch/utils/draw.py  \n",
            "  inflating: deep_sort_pytorch/utils/evaluation.py  \n",
            "  inflating: deep_sort_pytorch/utils/io.py  \n",
            "  inflating: deep_sort_pytorch/utils/json_logger.py  \n",
            "  inflating: deep_sort_pytorch/utils/log.py  \n",
            "  inflating: deep_sort_pytorch/utils/parser.py  \n",
            "  inflating: deep_sort_pytorch/utils/tools.py  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run the Script for Segmentation with DeepSORT Tracking"
      ],
      "metadata": {
        "id": "zm1MWVPqRI3g"
      },
      "id": "zm1MWVPqRI3g"
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_sort_pytorch.utils.parser import get_config\n",
        "from deep_sort_pytorch.deep_sort import DeepSort\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "wod3KuFS26cA"
      },
      "id": "wod3KuFS26cA",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e1a4ffd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e1a4ffd",
        "outputId": "d647fab4-69ba-4cbc-d3fb-ce96e2f51272",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Area: 621835.0\n",
            "Normalized Area: 351353.57595670526\n",
            "Processing frame 2 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 81.8ms\n",
            "Speed: 2.4ms preprocess, 81.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 4 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 40.7ms\n",
            "Speed: 2.3ms preprocess, 40.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 6 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 80.8ms\n",
            "Speed: 2.7ms preprocess, 80.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 8 / 6939\n",
            "\n",
            "0: 384x640 24 persons, 80.8ms\n",
            "Speed: 2.4ms preprocess, 80.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 10 / 6939\n",
            "\n",
            "0: 384x640 23 persons, 79.1ms\n",
            "Speed: 2.7ms preprocess, 79.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 12 / 6939\n",
            "\n",
            "0: 384x640 23 persons, 79.0ms\n",
            "Speed: 2.5ms preprocess, 79.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 14 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 80.8ms\n",
            "Speed: 2.6ms preprocess, 80.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 16 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 74.1ms\n",
            "Speed: 2.3ms preprocess, 74.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 18 / 6939\n",
            "\n",
            "0: 384x640 20 persons, 74.1ms\n",
            "Speed: 2.6ms preprocess, 74.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 20 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 77.7ms\n",
            "Speed: 2.5ms preprocess, 77.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 22 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 80.8ms\n",
            "Speed: 2.4ms preprocess, 80.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 24 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 80.8ms\n",
            "Speed: 2.7ms preprocess, 80.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 26 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 80.8ms\n",
            "Speed: 2.8ms preprocess, 80.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 28 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 80.9ms\n",
            "Speed: 2.4ms preprocess, 80.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 30 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 71.1ms\n",
            "Speed: 2.2ms preprocess, 71.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 32 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 71.2ms\n",
            "Speed: 2.4ms preprocess, 71.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 34 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 71.2ms\n",
            "Speed: 2.6ms preprocess, 71.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 36 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 66.1ms\n",
            "Speed: 2.8ms preprocess, 66.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 38 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 64.9ms\n",
            "Speed: 2.5ms preprocess, 64.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 40 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 64.9ms\n",
            "Speed: 2.5ms preprocess, 64.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 42 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 64.9ms\n",
            "Speed: 2.2ms preprocess, 64.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 44 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 80.8ms\n",
            "Speed: 2.3ms preprocess, 80.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 46 / 6939\n",
            "\n",
            "0: 384x640 14 persons, 80.8ms\n",
            "Speed: 2.2ms preprocess, 80.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 48 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 80.8ms\n",
            "Speed: 2.4ms preprocess, 80.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 50 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 80.9ms\n",
            "Speed: 2.3ms preprocess, 80.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 52 / 6939\n",
            "\n",
            "0: 384x640 21 persons, 74.1ms\n",
            "Speed: 2.6ms preprocess, 74.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 54 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 78.0ms\n",
            "Speed: 2.3ms preprocess, 78.0ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 56 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 80.8ms\n",
            "Speed: 2.4ms preprocess, 80.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 58 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 80.8ms\n",
            "Speed: 2.4ms preprocess, 80.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 60 / 6939\n",
            "\n",
            "0: 384x640 20 persons, 80.8ms\n",
            "Speed: 2.6ms preprocess, 80.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 62 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 66.1ms\n",
            "Speed: 2.4ms preprocess, 66.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 64 / 6939\n",
            "\n",
            "0: 384x640 13 persons, 66.1ms\n",
            "Speed: 2.2ms preprocess, 66.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 66 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 66.0ms\n",
            "Speed: 2.4ms preprocess, 66.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 68 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 80.8ms\n",
            "Speed: 2.2ms preprocess, 80.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 70 / 6939\n",
            "\n",
            "0: 384x640 14 persons, 77.3ms\n",
            "Speed: 2.4ms preprocess, 77.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 72 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 69.8ms\n",
            "Speed: 2.4ms preprocess, 69.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 74 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 69.8ms\n",
            "Speed: 2.2ms preprocess, 69.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 76 / 6939\n",
            "\n",
            "0: 384x640 20 persons, 76.6ms\n",
            "Speed: 2.3ms preprocess, 76.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 78 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 80.8ms\n",
            "Speed: 2.4ms preprocess, 80.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 80 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 67.2ms\n",
            "Speed: 2.5ms preprocess, 67.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 82 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 64.9ms\n",
            "Speed: 2.5ms preprocess, 64.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 84 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 64.9ms\n",
            "Speed: 2.3ms preprocess, 64.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 86 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 64.9ms\n",
            "Speed: 2.3ms preprocess, 64.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 88 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 80.8ms\n",
            "Speed: 2.6ms preprocess, 80.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 90 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 71.2ms\n",
            "Speed: 2.6ms preprocess, 71.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 92 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 71.2ms\n",
            "Speed: 2.4ms preprocess, 71.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 94 / 6939\n",
            "\n",
            "0: 384x640 13 persons, 80.8ms\n",
            "Speed: 2.5ms preprocess, 80.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 96 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 80.8ms\n",
            "Speed: 2.5ms preprocess, 80.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 98 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 80.8ms\n",
            "Speed: 2.6ms preprocess, 80.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 100 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 67.2ms\n",
            "Speed: 2.4ms preprocess, 67.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 102 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 64.9ms\n",
            "Speed: 2.4ms preprocess, 64.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 104 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 64.9ms\n",
            "Speed: 2.4ms preprocess, 64.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 106 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 80.8ms\n",
            "Speed: 2.3ms preprocess, 80.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 108 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 67.2ms\n",
            "Speed: 2.5ms preprocess, 67.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 110 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 64.9ms\n",
            "Speed: 2.4ms preprocess, 64.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 112 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 64.9ms\n",
            "Speed: 2.3ms preprocess, 64.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 114 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 64.9ms\n",
            "Speed: 2.4ms preprocess, 64.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 116 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 75.7ms\n",
            "Speed: 2.4ms preprocess, 75.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 118 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 80.8ms\n",
            "Speed: 2.6ms preprocess, 80.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 120 / 6939\n",
            "\n",
            "0: 384x640 14 persons, 80.8ms\n",
            "Speed: 2.5ms preprocess, 80.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 122 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 69.8ms\n",
            "Speed: 2.4ms preprocess, 69.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 124 / 6939\n",
            "\n",
            "0: 384x640 21 persons, 66.0ms\n",
            "Speed: 2.4ms preprocess, 66.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 126 / 6939\n",
            "\n",
            "0: 384x640 20 persons, 66.0ms\n",
            "Speed: 2.6ms preprocess, 66.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 128 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 80.8ms\n",
            "Speed: 2.4ms preprocess, 80.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 130 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 67.2ms\n",
            "Speed: 2.6ms preprocess, 67.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 132 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 64.9ms\n",
            "Speed: 2.4ms preprocess, 64.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 134 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 64.9ms\n",
            "Speed: 2.3ms preprocess, 64.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 136 / 6939\n",
            "\n",
            "0: 384x640 24 persons, 64.9ms\n",
            "Speed: 2.5ms preprocess, 64.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 138 / 6939\n",
            "\n",
            "0: 384x640 21 persons, 62.7ms\n",
            "Speed: 2.7ms preprocess, 62.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 140 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 62.9ms\n",
            "Speed: 2.8ms preprocess, 62.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 142 / 6939\n",
            "\n",
            "0: 384x640 13 persons, 75.6ms\n",
            "Speed: 2.5ms preprocess, 75.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 144 / 6939\n",
            "\n",
            "0: 384x640 14 persons, 68.5ms\n",
            "Speed: 2.3ms preprocess, 68.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 146 / 6939\n",
            "\n",
            "0: 384x640 20 persons, 68.5ms\n",
            "Speed: 2.3ms preprocess, 68.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 148 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 64.9ms\n",
            "Speed: 2.3ms preprocess, 64.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 150 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 64.9ms\n",
            "Speed: 2.2ms preprocess, 64.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 152 / 6939\n",
            "\n",
            "0: 384x640 20 persons, 65.0ms\n",
            "Speed: 2.4ms preprocess, 65.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 154 / 6939\n",
            "\n",
            "0: 384x640 14 persons, 64.9ms\n",
            "Speed: 2.2ms preprocess, 64.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 156 / 6939\n",
            "\n",
            "0: 384x640 14 persons, 64.9ms\n",
            "Speed: 2.3ms preprocess, 64.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 158 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 64.9ms\n",
            "Speed: 2.5ms preprocess, 64.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 160 / 6939\n",
            "\n",
            "0: 384x640 14 persons, 80.8ms\n",
            "Speed: 2.3ms preprocess, 80.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 162 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 80.8ms\n",
            "Speed: 2.5ms preprocess, 80.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 164 / 6939\n",
            "\n",
            "0: 384x640 12 persons, 80.8ms\n",
            "Speed: 2.5ms preprocess, 80.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 166 / 6939\n",
            "\n",
            "0: 384x640 14 persons, 69.8ms\n",
            "Speed: 2.6ms preprocess, 69.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 168 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 68.5ms\n",
            "Speed: 2.5ms preprocess, 68.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 170 / 6939\n",
            "\n",
            "0: 384x640 14 persons, 67.2ms\n",
            "Speed: 2.3ms preprocess, 67.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 172 / 6939\n",
            "\n",
            "0: 384x640 13 persons, 67.3ms\n",
            "Speed: 2.4ms preprocess, 67.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 174 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 67.3ms\n",
            "Speed: 2.6ms preprocess, 67.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 176 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 79.0ms\n",
            "Speed: 2.3ms preprocess, 79.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 178 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 67.2ms\n",
            "Speed: 2.5ms preprocess, 67.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 180 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 64.9ms\n",
            "Speed: 3.0ms preprocess, 64.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 182 / 6939\n",
            "\n",
            "0: 384x640 14 persons, 64.9ms\n",
            "Speed: 2.5ms preprocess, 64.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 184 / 6939\n",
            "\n",
            "0: 384x640 12 persons, 64.9ms\n",
            "Speed: 2.5ms preprocess, 64.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 186 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 79.0ms\n",
            "Speed: 2.6ms preprocess, 79.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 188 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 67.2ms\n",
            "Speed: 2.3ms preprocess, 67.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 190 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 80.9ms\n",
            "Speed: 2.4ms preprocess, 80.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 192 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 69.8ms\n",
            "Speed: 2.5ms preprocess, 69.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 194 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 69.8ms\n",
            "Speed: 2.1ms preprocess, 69.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 196 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 66.0ms\n",
            "Speed: 2.2ms preprocess, 66.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 198 / 6939\n",
            "\n",
            "0: 384x640 24 persons, 65.0ms\n",
            "Speed: 2.4ms preprocess, 65.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 200 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 65.0ms\n",
            "Speed: 2.5ms preprocess, 65.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 202 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 69.2ms\n",
            "Speed: 2.4ms preprocess, 69.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 204 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 80.8ms\n",
            "Speed: 2.5ms preprocess, 80.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 206 / 6939\n",
            "\n",
            "0: 384x640 23 persons, 77.3ms\n",
            "Speed: 2.4ms preprocess, 77.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 208 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 75.6ms\n",
            "Speed: 2.8ms preprocess, 75.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 210 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 75.6ms\n",
            "Speed: 2.5ms preprocess, 75.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 212 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 75.7ms\n",
            "Speed: 2.5ms preprocess, 75.7ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 214 / 6939\n",
            "\n",
            "0: 384x640 24 persons, 66.0ms\n",
            "Speed: 2.3ms preprocess, 66.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 216 / 6939\n",
            "\n",
            "0: 384x640 20 persons, 80.9ms\n",
            "Speed: 2.4ms preprocess, 80.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 218 / 6939\n",
            "\n",
            "0: 384x640 21 persons, 80.9ms\n",
            "Speed: 2.5ms preprocess, 80.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 220 / 6939\n",
            "\n",
            "0: 384x640 22 persons, 80.8ms\n",
            "Speed: 2.3ms preprocess, 80.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 222 / 6939\n",
            "\n",
            "0: 384x640 21 persons, 69.8ms\n",
            "Speed: 2.4ms preprocess, 69.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 224 / 6939\n",
            "\n",
            "0: 384x640 20 persons, 63.8ms\n",
            "Speed: 2.4ms preprocess, 63.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 226 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 63.8ms\n",
            "Speed: 2.5ms preprocess, 63.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 228 / 6939\n",
            "\n",
            "0: 384x640 23 persons, 63.8ms\n",
            "Speed: 2.5ms preprocess, 63.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 230 / 6939\n",
            "\n",
            "0: 384x640 13 persons, 79.6ms\n",
            "Speed: 2.2ms preprocess, 79.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 232 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 80.8ms\n",
            "Speed: 2.5ms preprocess, 80.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 234 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 67.3ms\n",
            "Speed: 2.5ms preprocess, 67.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 236 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 64.9ms\n",
            "Speed: 2.3ms preprocess, 64.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 238 / 6939\n",
            "\n",
            "0: 384x640 21 persons, 64.9ms\n",
            "Speed: 2.7ms preprocess, 64.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 240 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 63.8ms\n",
            "Speed: 2.3ms preprocess, 63.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 242 / 6939\n",
            "\n",
            "0: 384x640 21 persons, 63.8ms\n",
            "Speed: 2.5ms preprocess, 63.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 244 / 6939\n",
            "\n",
            "0: 384x640 20 persons, 80.9ms\n",
            "Speed: 2.7ms preprocess, 80.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 246 / 6939\n",
            "\n",
            "0: 384x640 22 persons, 66.1ms\n",
            "Speed: 2.4ms preprocess, 66.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 248 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 66.0ms\n",
            "Speed: 2.3ms preprocess, 66.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 250 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 80.8ms\n",
            "Speed: 2.5ms preprocess, 80.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 252 / 6939\n",
            "\n",
            "0: 384x640 10 persons, 67.2ms\n",
            "Speed: 2.5ms preprocess, 67.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 254 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 67.2ms\n",
            "Speed: 2.1ms preprocess, 67.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 256 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 80.8ms\n",
            "Speed: 2.6ms preprocess, 80.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 258 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 80.8ms\n",
            "Speed: 2.3ms preprocess, 80.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 260 / 6939\n",
            "\n",
            "0: 384x640 14 persons, 80.8ms\n",
            "Speed: 2.6ms preprocess, 80.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 262 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 69.8ms\n",
            "Speed: 2.4ms preprocess, 69.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 264 / 6939\n",
            "\n",
            "0: 384x640 12 persons, 65.0ms\n",
            "Speed: 2.6ms preprocess, 65.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 266 / 6939\n",
            "\n",
            "0: 384x640 12 persons, 64.9ms\n",
            "Speed: 2.5ms preprocess, 64.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 268 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 79.0ms\n",
            "Speed: 2.5ms preprocess, 79.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 270 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 66.1ms\n",
            "Speed: 2.7ms preprocess, 66.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 272 / 6939\n",
            "\n",
            "0: 384x640 14 persons, 66.1ms\n",
            "Speed: 3.1ms preprocess, 66.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 274 / 6939\n",
            "\n",
            "0: 384x640 14 persons, 80.8ms\n",
            "Speed: 2.4ms preprocess, 80.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 276 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 69.8ms\n",
            "Speed: 2.6ms preprocess, 69.8ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 278 / 6939\n",
            "\n",
            "0: 384x640 11 persons, 80.8ms\n",
            "Speed: 2.9ms preprocess, 80.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 280 / 6939\n",
            "\n",
            "0: 384x640 12 persons, 69.8ms\n",
            "Speed: 2.3ms preprocess, 69.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 282 / 6939\n",
            "\n",
            "0: 384x640 9 persons, 68.5ms\n",
            "Speed: 2.3ms preprocess, 68.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 284 / 6939\n",
            "\n",
            "0: 384x640 12 persons, 68.5ms\n",
            "Speed: 2.4ms preprocess, 68.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 286 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 80.8ms\n",
            "Speed: 2.4ms preprocess, 80.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 288 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 69.9ms\n",
            "Speed: 3.1ms preprocess, 69.9ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 290 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 68.5ms\n",
            "Speed: 2.6ms preprocess, 68.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 292 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 68.5ms\n",
            "Speed: 3.3ms preprocess, 68.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 294 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 66.1ms\n",
            "Speed: 2.5ms preprocess, 66.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 296 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 64.9ms\n",
            "Speed: 2.5ms preprocess, 64.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 298 / 6939\n",
            "\n",
            "0: 384x640 13 persons, 64.9ms\n",
            "Speed: 2.4ms preprocess, 64.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 300 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 80.9ms\n",
            "Speed: 3.2ms preprocess, 80.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 302 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 67.3ms\n",
            "Speed: 3.2ms preprocess, 67.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 304 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 67.2ms\n",
            "Speed: 2.6ms preprocess, 67.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 306 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 80.8ms\n",
            "Speed: 2.6ms preprocess, 80.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 308 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 80.9ms\n",
            "Speed: 2.8ms preprocess, 80.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 310 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 80.9ms\n",
            "Speed: 2.7ms preprocess, 80.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 312 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 67.2ms\n",
            "Speed: 2.4ms preprocess, 67.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 314 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 72.0ms\n",
            "Speed: 2.8ms preprocess, 72.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 316 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 80.8ms\n",
            "Speed: 2.6ms preprocess, 80.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 318 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 80.9ms\n",
            "Speed: 2.3ms preprocess, 80.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 320 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 80.8ms\n",
            "Speed: 2.6ms preprocess, 80.8ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 322 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 79.0ms\n",
            "Speed: 2.3ms preprocess, 79.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 324 / 6939\n",
            "\n",
            "0: 384x640 21 persons, 79.0ms\n",
            "Speed: 2.5ms preprocess, 79.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 326 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 65.0ms\n",
            "Speed: 3.2ms preprocess, 65.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 328 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 64.9ms\n",
            "Speed: 2.7ms preprocess, 64.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 330 / 6939\n",
            "\n",
            "0: 384x640 14 persons, 79.1ms\n",
            "Speed: 2.4ms preprocess, 79.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 332 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 80.9ms\n",
            "Speed: 2.5ms preprocess, 80.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 334 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 80.8ms\n",
            "Speed: 2.7ms preprocess, 80.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 336 / 6939\n",
            "\n",
            "0: 384x640 21 persons, 80.9ms\n",
            "Speed: 2.5ms preprocess, 80.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 338 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 66.0ms\n",
            "Speed: 2.7ms preprocess, 66.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 340 / 6939\n",
            "\n",
            "0: 384x640 20 persons, 64.9ms\n",
            "Speed: 2.6ms preprocess, 64.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 342 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 64.9ms\n",
            "Speed: 2.4ms preprocess, 64.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 344 / 6939\n",
            "\n",
            "0: 384x640 21 persons, 80.8ms\n",
            "Speed: 2.6ms preprocess, 80.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 346 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 64.9ms\n",
            "Speed: 2.4ms preprocess, 64.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 348 / 6939\n",
            "\n",
            "0: 384x640 22 persons, 64.9ms\n",
            "Speed: 2.5ms preprocess, 64.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 350 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 80.8ms\n",
            "Speed: 2.5ms preprocess, 80.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 352 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 80.9ms\n",
            "Speed: 2.4ms preprocess, 80.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 354 / 6939\n",
            "\n",
            "0: 384x640 14 persons, 69.8ms\n",
            "Speed: 2.6ms preprocess, 69.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 356 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 80.9ms\n",
            "Speed: 2.6ms preprocess, 80.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 358 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 80.9ms\n",
            "Speed: 2.7ms preprocess, 80.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 360 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 69.8ms\n",
            "Speed: 2.3ms preprocess, 69.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 362 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 80.9ms\n",
            "Speed: 2.2ms preprocess, 80.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 364 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 80.9ms\n",
            "Speed: 2.6ms preprocess, 80.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 366 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 67.2ms\n",
            "Speed: 2.3ms preprocess, 67.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 368 / 6939\n",
            "\n",
            "0: 384x640 13 persons, 80.8ms\n",
            "Speed: 2.3ms preprocess, 80.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 370 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 80.8ms\n",
            "Speed: 2.9ms preprocess, 80.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 372 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 77.3ms\n",
            "Speed: 2.4ms preprocess, 77.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 374 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 68.5ms\n",
            "Speed: 2.6ms preprocess, 68.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 376 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 67.3ms\n",
            "Speed: 2.3ms preprocess, 67.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 378 / 6939\n",
            "\n",
            "0: 384x640 16 persons, 67.3ms\n",
            "Speed: 2.8ms preprocess, 67.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 380 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 80.8ms\n",
            "Speed: 2.5ms preprocess, 80.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 382 / 6939\n",
            "\n",
            "0: 384x640 17 persons, 67.2ms\n",
            "Speed: 2.2ms preprocess, 67.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 384 / 6939\n",
            "\n",
            "0: 384x640 15 persons, 67.2ms\n",
            "Speed: 2.2ms preprocess, 67.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 386 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 80.8ms\n",
            "Speed: 2.2ms preprocess, 80.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 388 / 6939\n",
            "\n",
            "0: 384x640 19 persons, 66.0ms\n",
            "Speed: 2.5ms preprocess, 66.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 390 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 66.0ms\n",
            "Speed: 2.4ms preprocess, 66.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 392 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 64.9ms\n",
            "Speed: 2.6ms preprocess, 64.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 394 / 6939\n",
            "\n",
            "0: 384x640 18 persons, 64.9ms\n",
            "Speed: 2.2ms preprocess, 64.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate polygon area using Shoelace formula\n",
        "def calculate_area(points):\n",
        "    x = points[:, 0]\n",
        "    y = points[:, 1]\n",
        "    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n",
        "\n",
        "# Function to scale and normalize the area\n",
        "def normalize_area(area, original_size, target_size):\n",
        "    scale_factor = (target_size[0] / original_size[0]) * (target_size[1] / original_size[1])\n",
        "    return area * scale_factor\n",
        "\n",
        "# Original image points\n",
        "points = np.array([\n",
        "    [2193, 586],\n",
        "    [2448, 630],\n",
        "    [332, 1349],\n",
        "    [2356, 1336]\n",
        "], dtype='float32')\n",
        "\n",
        "original_size = (2561, 1433)  # Original video size\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get the frame dimensions\n",
        "video_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "video_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "target_size = (video_width, video_height)\n",
        "\n",
        "area = calculate_area(points)\n",
        "normalized_area = normalize_area(area, original_size, target_size)\n",
        "\n",
        "print(f\"Original Area: {area}\")\n",
        "print(f\"Normalized Area: {normalized_area}\")\n",
        "\n",
        "# Set parameters\n",
        "skip = 2\n",
        "output_dir = \"output\"\n",
        "\n",
        "# Function to initialize the DeepSORT tracker\n",
        "def init_tracker():\n",
        "    cfg = get_config()\n",
        "    cfg.merge_from_file(\"deep_sort_pytorch/configs/deep_sort.yaml\")\n",
        "\n",
        "    deepsort = DeepSort(cfg.DEEPSORT.REID_CKPT,\n",
        "                        max_dist=cfg.DEEPSORT.MAX_DIST,\n",
        "                        min_confidence=cfg.DEEPSORT.MIN_CONFIDENCE,\n",
        "                        nms_max_overlap=cfg.DEEPSORT.NMS_MAX_OVERLAP,\n",
        "                        max_iou_distance=cfg.DEEPSORT.MAX_IOU_DISTANCE,\n",
        "                        max_age=cfg.DEEPSORT.MAX_AGE,\n",
        "                        n_init=cfg.DEEPSORT.N_INIT,\n",
        "                        nn_budget=cfg.DEEPSORT.NN_BUDGET,\n",
        "                        use_cuda=True)\n",
        "    return deepsort\n",
        "\n",
        "# Load the YOLO model for person detection\n",
        "model_person = YOLO(\"/content/yolo11x-seg.pt\").to(\"cuda\")  # Ensure this is your correct device\n",
        "\n",
        "# Initialize Roboflow client for shadow detection\n",
        "CLIENT = InferenceHTTPClient(api_url=\"https://detect.roboflow.com\", api_key=\"2nHLYGbdwz0EHlteoHxo\")\n",
        "\n",
        "# Create a subfolder for each video in the output directory\n",
        "video_output_dir = os.path.join(output_dir, file_name)\n",
        "if not os.path.exists(video_output_dir):\n",
        "    os.makedirs(video_output_dir)\n",
        "\n",
        "output_video_path = os.path.join(video_output_dir, f\"{file_name}_human-and-shade-tracking.avi\")\n",
        "csv_file = os.path.join(video_output_dir, f\"{file_name}_detections.csv\")\n",
        "\n",
        "# Initialize the DeepSORT tracker\n",
        "deepsort = init_tracker()\n",
        "\n",
        "# Dictionary to store trails\n",
        "trails = {}\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video.\")\n",
        "    exit()\n",
        "\n",
        "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*\"MJPG\"), int(video.get(cv2.CAP_PROP_FPS)), target_size)\n",
        "\n",
        "# Initialize CSV file with headers\n",
        "csv_headers = ['frame', 'person_id', 'bottom_mid_x', 'bottom_mid_y', 'in_shadow', 'shadow_area', 'proportion_of_shade']\n",
        "with open(csv_file, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(csv_headers)\n",
        "\n",
        "frame_count = 0\n",
        "consecutive_person_id = 0\n",
        "person_id_mapping = {}\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, frame = cap.read()\n",
        "    if not success:\n",
        "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "    if frame_count % skip == 0:\n",
        "        print(f\"Processing frame {frame_count} / {total_frames}\")  # Display progress\n",
        "\n",
        "        shadow_result = CLIENT.infer(frame, model_id=\"shade-detection/6\")\n",
        "        boundaries = []\n",
        "\n",
        "        if 'predictions' in shadow_result:\n",
        "            predictions = shadow_result['predictions']\n",
        "            for prediction in predictions:\n",
        "                points = prediction.get(\"points\", [])\n",
        "                if points:\n",
        "                    boundary = [(int(point['x']), int(point['y'])) for point in points]\n",
        "                    boundaries.append(boundary)\n",
        "\n",
        "        annotator = Annotator(frame, line_width=2)\n",
        "        people_in_shadow = 0\n",
        "        people_outside_shadow = 0\n",
        "        total_people = 0\n",
        "        detections = []\n",
        "\n",
        "        shade_area = 0\n",
        "        # Draw the shadow polygon\n",
        "        for boundary in boundaries:\n",
        "            boundary_points = np.array(boundary, np.int32).reshape((-1, 1, 2))\n",
        "            cv2.polylines(frame, [boundary_points], isClosed=True, color=(255, 0, 0), thickness=2)\n",
        "            shade_area = cv2.contourArea(boundary_points)\n",
        "\n",
        "        # Calculate the proportion of shade using normalized area\n",
        "        proportion_of_shade = shade_area / normalized_area if normalized_area != 0 else 0\n",
        "\n",
        "        # People detection\n",
        "        person_results = model_person(frame, classes=[0], conf=0.05, show=False)\n",
        "        if person_results and len(person_results) > 0:\n",
        "            boxes = person_results[0].boxes\n",
        "\n",
        "            if boxes and len(boxes) > 0:\n",
        "                xyxy = boxes.xyxy.cpu().numpy()\n",
        "                confidences = boxes.conf.cpu().numpy()\n",
        "                oids = boxes.cls.cpu().numpy()\n",
        "\n",
        "                # Convert to the format expected by DeepSORT\n",
        "                bbox_xywh = []\n",
        "                for box in xyxy:\n",
        "                    x1, y1, x2, y2 = box\n",
        "                    xc = (x1 + x2) / 2\n",
        "                    yc = (y1 + y2) / 2\n",
        "                    w = x2 - x1\n",
        "                    h = y2 - y1\n",
        "                    bbox_xywh.append([xc, yc, w, h])\n",
        "\n",
        "                bbox_xywh = np.array(bbox_xywh)\n",
        "\n",
        "                # Update DeepSORT tracker with detected boxes\n",
        "                outputs = deepsort.update(bbox_xywh, confidences, oids, frame)\n",
        "\n",
        "                for output in outputs:\n",
        "                    x1, y1, x2, y2, track_id, track_oid = output\n",
        "                    bottom_mid_x = int((x1 + x2) // 2)\n",
        "                    bottom_mid_y = int(y2)\n",
        "                    in_shadow = False\n",
        "\n",
        "                    # Map the original track_id to consecutive_person_id\n",
        "                    if track_id not in person_id_mapping:\n",
        "                        person_id_mapping[track_id] = consecutive_person_id\n",
        "                        consecutive_person_id += 1\n",
        "\n",
        "                    # Use the mapped consecutive_person_id\n",
        "                    person_id = person_id_mapping[track_id]\n",
        "\n",
        "                    for boundary in boundaries:\n",
        "                        if cv2.pointPolygonTest(np.array(boundary, np.int32), (bottom_mid_x, bottom_mid_y), False) >= 0:\n",
        "                            in_shadow = True\n",
        "                            people_in_shadow += 1\n",
        "                            break\n",
        "                    else:\n",
        "                        people_outside_shadow += 1\n",
        "\n",
        "                    detections.append([frame_count, person_id, bottom_mid_x, bottom_mid_y, in_shadow, shade_area, proportion_of_shade])\n",
        "                    total_people += 1\n",
        "\n",
        "                    # Store the position for trails\n",
        "                    if track_id not in trails:\n",
        "                        trails[track_id] = deque(maxlen=30)\n",
        "                    trails[track_id].append((bottom_mid_x, bottom_mid_y))\n",
        "\n",
        "                    # Draw the trails\n",
        "                    for i in range(1, len(trails[track_id])):\n",
        "                        if trails[track_id][i - 1] is None or trails[track_id][i] is None:\n",
        "                            continue\n",
        "                        cv2.line(frame, trails[track_id][i - 1], trails[track_id][i], (0, 255, 0), 2)\n",
        "\n",
        "                    color = (0, 255, 0)  # You can use your colors function if defined\n",
        "                    label = f\"ID {person_id} {'in shadow' if in_shadow else 'out shadow'}\"\n",
        "                    annotator.box_label([x1, y1, x2, y2], label=label, color=color)\n",
        "                    cv2.circle(frame, (bottom_mid_x, bottom_mid_y), 5, (0, 0, 255), -1)\n",
        "\n",
        "        # Write the results to CSV\n",
        "        with open(csv_file, 'a', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerows(detections)\n",
        "\n",
        "        # Display the statistics results on the frame\n",
        "        text = f\"Frame {frame_count}: Total: {total_people}, In Shadow: {people_in_shadow}, Outside Shadow: {people_outside_shadow}\"\n",
        "        cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "        # Write the original frame\n",
        "        out.write(frame)\n",
        "\n",
        "        #COMMENT OUT IF ON COLAB, but can display progress\n",
        "        #cv2_imshow(\"instance-segmentation-object-tracking\", frame)  # Display the frame using cv2_imshow\n",
        "        #if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "        #    break\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "print(f\"Processing complete. Results saved in {output_video_path} and {csv_file}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis"
      ],
      "metadata": {
        "id": "CQRNZ1VmRSoI"
      },
      "id": "CQRNZ1VmRSoI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating a document to store all graphs"
      ],
      "metadata": {
        "id": "PtmiUDFXQia7"
      },
      "id": "PtmiUDFXQia7"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "csv_dir = os.path.dirname(csv_file)\n",
        "\n",
        "# Create a PDF document\n",
        "pdf_file = os.path.join(csv_dir, f\"{file_name}_all_plots.pdf\")\n",
        "pdf = PdfPages(pdf_file)"
      ],
      "metadata": {
        "id": "QjTvfVn8QhyF"
      },
      "id": "QjTvfVn8QhyF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Count of People"
      ],
      "metadata": {
        "id": "nnCMyMD4Qlvf"
      },
      "id": "nnCMyMD4Qlvf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8569ab5d",
      "metadata": {
        "id": "8569ab5d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "# Initialize dictionaries to count the number of people in and out of shadow for each frame\n",
        "num_in_shadow = {}\n",
        "num_out_shadow = {}\n",
        "\n",
        "# Count the number of people in and out of shadow for each frame\n",
        "for frame in data['frame'].unique():\n",
        "    count_in_shadow = len(data[(data['in_shadow'] == True) & (data['frame'] == frame)])\n",
        "    count_out_shadow = len(data[(data['in_shadow'] == False) & (data['frame'] == frame)])\n",
        "    num_in_shadow[frame] = count_in_shadow\n",
        "    num_out_shadow[frame] = count_out_shadow\n",
        "\n",
        "# Create a new DataFrame from the counts\n",
        "count_df = pd.DataFrame({\n",
        "    'Frame': num_in_shadow.keys(),\n",
        "    'in_shadow': num_in_shadow.values(),\n",
        "    'out_shadow': num_out_shadow.values()\n",
        "})\n",
        "\n",
        "print(count_df)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(count_df['Frame'], count_df['in_shadow'], label='# in shadow')\n",
        "ax.plot(count_df['Frame'], count_df['out_shadow'], label='# out shadow')\n",
        "ax.legend()\n",
        "\n",
        "ax.set_xlabel('Frame')\n",
        "ax.set_ylabel('Number of people')\n",
        "ax.set_title(\"Counts of People In and Out of Shadow per Frame\")\n",
        "ax.grid(True)\n",
        "\n",
        "pdf.savefig(fig)\n",
        "plt.show()\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "id": "rkh8WfR45cjw"
      },
      "id": "rkh8WfR45cjw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Moving Average"
      ],
      "metadata": {
        "id": "kYHSG0SxRjcu"
      },
      "id": "kYHSG0SxRjcu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a257d61",
      "metadata": {
        "id": "5a257d61"
      },
      "outputs": [],
      "source": [
        "# Calculate the moving average for 'out_shadow' and 'in_shadow'\n",
        "window_size = 400\n",
        "count_df['ma_out_shadow'] = count_df['out_shadow'].rolling(window=window_size).mean()\n",
        "count_df['ma_in_shadow'] = count_df['in_shadow'].rolling(window=window_size).mean()\n",
        "\n",
        "# Plot the moving averages\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(count_df['Frame'], count_df['ma_in_shadow'], label='Moving Average In Shadow')\n",
        "ax.plot(count_df['Frame'], count_df['ma_out_shadow'], label='Moving Average Out Shadow')\n",
        "ax.legend()\n",
        "ax.set_xlabel('Frame')\n",
        "ax.set_ylabel('Number of people')\n",
        "ax.set_title(f'Moving Average ({window_size}) of People In and Out of Shadow')\n",
        "\n",
        "pdf.savefig(fig)\n",
        "plt.show()\n",
        "plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Shade Evolution"
      ],
      "metadata": {
        "id": "emdgbshY65Pe"
      },
      "id": "emdgbshY65Pe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the ratio of people in shade vs. out of shade\n",
        "count_df['ratio_in_vs_out'] = count_df['in_shadow'] / count_df['out_shadow']\n",
        "\n",
        "# Replace NaN values with zeros (or another value if appropriate)\n",
        "count_df['ratio_in_vs_out'] = count_df['ratio_in_vs_out'].fillna(0)\n",
        "\n",
        "# Plot the ratio\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(count_df['Frame'], count_df['ratio_in_vs_out'], label='Ratio In vs Out Shadow', color='purple')\n",
        "ax.set_xlabel('Frame')\n",
        "ax.set_ylabel('Ratio of people in shadow vs out of shadow')\n",
        "ax.set_title('Ratio of People In and Out of Shadow Over Time')\n",
        "ax.legend()\n",
        "\n",
        "# Save the current plot to the PDF\n",
        "pdf.savefig(fig)\n",
        "plt.show()\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "id": "Ojps34LDtNW4"
      },
      "id": "Ojps34LDtNW4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the ratio of people in shade vs. out of shade\n",
        "count_df['ratio_in_vs_out'] = count_df['in_shadow'] / count_df['out_shadow']\n",
        "\n",
        "# Replace NaN and infinite values with zeros (or another value if appropriate)\n",
        "count_df['ratio_in_vs_out'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "count_df['ratio_in_vs_out'] = count_df['ratio_in_vs_out'].fillna(0)\n",
        "\n",
        "# Calculate the moving average for the ratio\n",
        "window_size = 400\n",
        "count_df['ma_ratio_in_vs_out'] = count_df['ratio_in_vs_out'].rolling(window=window_size).mean()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(count_df['Frame'], count_df['ma_ratio_in_vs_out'], label=f'Moving Average Ratio In vs Out Shadow (window={window_size})', color='purple')\n",
        "ax.set_xlabel('Frame')\n",
        "ax.set_ylabel('Moving Average Ratio of people in shadow vs out of shadow')\n",
        "ax.set_title(f'Moving Average Ratio of People In and Out of Shadow Over Time (window={window_size})')\n",
        "ax.legend()\n",
        "\n",
        "# Save the current plot to the PDF\n",
        "pdf.savefig(fig)\n",
        "plt.show()\n",
        "plt.close(fig)\n",
        "\n",
        "print"
      ],
      "metadata": {
        "id": "ygLFlVk9T9a8"
      },
      "id": "ygLFlVk9T9a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the 'frame' and 'shade_area' columns\n",
        "shade_data = data[['frame', 'shadow_area', 'proportion_of_shade']]\n",
        "\n",
        "# Remove duplicate rows\n",
        "shade_data = shade_data.drop_duplicates()\n",
        "\n",
        "# Sort the data by 'frame' to ensure the plot is in the correct order\n",
        "shade_data = shade_data.sort_values(by='frame')\n",
        "\n",
        "# Calculate the moving average with a window size of your choice (e.g., 5 frames)\n",
        "window_size = 400\n",
        "shade_data['shadow_area_moving_avg'] = shade_data['shadow_area'].rolling(window=window_size).mean()\n",
        "\n",
        "output_dir = \"output\"\n",
        "\n",
        "# Create a new subfolder with the video name inside the output directory\n",
        "video_output_dir = os.path.join(output_dir, file_name)\n",
        "if not os.path.exists(video_output_dir):\n",
        "    os.makedirs(video_output_dir)\n",
        "\n",
        "# Define the new path for the data file in the new subfolder\n",
        "shade_data_file = os.path.join(video_output_dir, f\"{file_name}_shadeIndexProportions.csv\")\n",
        "shade_data.to_csv(shade_data_file, index=False)\n",
        "\n",
        "print(f\"Data file saved at: {shade_data_file}\")\n",
        "\n",
        "# Plot the original shade area and its moving average\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "#ax.plot(data['frame'], data['shadow_area'], marker='o', linestyle='-', color='b', label='Shade Area')\n",
        "ax.plot(shade_data['frame'], shade_data['shadow_area_moving_avg'], marker='', linestyle='-', color='r', label=f'Moving Average (window={window_size})')\n",
        "ax.set_xlabel('Frame Number')\n",
        "ax.set_ylabel('Shadow Area (pixels^2)')\n",
        "ax.set_title('Evolution of Shadow Area (with Moving Average)')\n",
        "ax.legend()\n",
        "ax.grid(True)\n",
        "\n",
        "# Save the current plot to the PDF\n",
        "pdf.savefig(fig)\n",
        "plt.show()\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "id": "6aWeagUg63-u"
      },
      "id": "6aWeagUg63-u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Overlaying Graphs"
      ],
      "metadata": {
        "id": "UaAz-dkWKJID"
      },
      "id": "UaAz-dkWKJID"
    },
    {
      "cell_type": "code",
      "source": [
        "# axes\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "ax1.plot(shade_data['frame'], shade_data['shadow_area_moving_avg'], marker='', linestyle='-', color='r', label=f'Moving Average (window={window_size})')\n",
        "ax1.tick_params(axis='y', labelcolor='b')\n",
        "ax1.set_ylabel('Shadow Area Moving Average', color='b')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax2.plot(count_df['Frame'], count_df['ma_ratio_in_vs_out'], label=f'Moving Average Ratio In vs Out Shadow (window={window_size})', color='purple')\n",
        "ax2.tick_params(axis='y', labelcolor='r')\n",
        "ax2.set_ylabel('Ratio In vs Out Shadow', color='r')\n",
        "\n",
        "fig.legend(loc='upper right')\n",
        "plt.title('Moving Averages of Shadow Area and Ratio of In vs. Out shadow')\n",
        "\n",
        "pdf.savefig(fig)\n",
        "plt.show()\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "id": "P0xR7GFPKIkn"
      },
      "id": "P0xR7GFPKIkn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Geocoordinates - download TIF"
      ],
      "metadata": {
        "id": "PcSKzyOGqIUR"
      },
      "id": "PcSKzyOGqIUR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "not accurate yet (if you plot the geocoordinates, they form a graph with the same perspective as the image, and it should be expected to form a rectangular shape like the TIF)"
      ],
      "metadata": {
        "id": "ydIiBmdV0t9d"
      },
      "id": "ydIiBmdV0t9d"
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=119tkpmy3u13tydRT83xoKgXY2trbX8Fk&export=download"
      ],
      "metadata": {
        "id": "JWialSURqK5j"
      },
      "id": "JWialSURqK5j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from osgeo import gdal\n",
        "import pandas as pd\n",
        "\n",
        "def pixel_to_latlon(tif_file, pixel_x, pixel_y, img_width, img_height):\n",
        "    dataset = gdal.Open(tif_file)\n",
        "    geotransform = dataset.GetGeoTransform()\n",
        "\n",
        "    # Normalize pixel coordinates\n",
        "    norm_pixel_x = pixel_x / img_width\n",
        "    norm_pixel_y = pixel_y / img_height\n",
        "\n",
        "    # Get the origin coordinates\n",
        "    x_origin = geotransform[0]\n",
        "    y_origin = geotransform[3]\n",
        "\n",
        "    # Get the pixel size\n",
        "    pixel_width = geotransform[1]\n",
        "    pixel_height = geotransform[5]\n",
        "\n",
        "    # Calculate the latitude and longitude\n",
        "    lat = y_origin + norm_pixel_y * img_height * pixel_height\n",
        "    lon = x_origin + norm_pixel_x * img_width * pixel_width\n",
        "\n",
        "    return lat, lon\n",
        "\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get the frame dimensions\n",
        "video_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "video_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Iterate through each row in the CSV\n",
        "geo_coords = data.apply(\n",
        "    lambda row: pixel_to_latlon('/content/streetview.tif', row['bottom_mid_x'], row['bottom_mid_y'], video_width, video_height), axis=1\n",
        ")\n",
        "\n",
        "# Split the geocoordinates into separate columns\n",
        "data['latitude'] = geo_coords.apply(lambda coord: coord[0])\n",
        "data['longitude'] = geo_coords.apply(lambda coord: coord[1])\n",
        "\n",
        "# Save the transformed coordinates and geocoordinates to a new CSV file\n",
        "# Save to a new CSV file\n",
        "\n",
        "transformed_output_file = os.path.join(video_output_dir, f\"{file_name}_geoData.csv\")\n",
        "data.to_csv(transformed_output_file, index=False)\n",
        "\n",
        "print(f\"Transformed CSV saved to {transformed_output_file}\")"
      ],
      "metadata": {
        "id": "4bzv8AraHXj3"
      },
      "id": "4bzv8AraHXj3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filter"
      ],
      "metadata": {
        "id": "B2XrxMhLIIi2"
      },
      "id": "B2XrxMhLIIi2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the csv\n",
        "data = pd.read_csv(transformed_output_file)\n",
        "\n",
        "# Define the minimum presence duration (in frames) to consider a track segment as valid\n",
        "MIN_PRESENCE_DURATION = 30  # Adjust this based on your needs\n",
        "\n",
        "# Define the maximum allowed displacement (in pixels)\n",
        "MAX_DISPLACEMENT = 50  # Adjust this threshold based on your needs\n",
        "\n",
        "# Define the maximum gap between frames to consider a continuous segment\n",
        "MAX_FRAME_GAP = 100  # Adjust this threshold based on your needs\n",
        "\n",
        "# Function to process each group\n",
        "def process_group(x):\n",
        "    # Extract the list of tuples\n",
        "    coordinates_and_shadow = list(zip(x['frame'], x['bottom_mid_x'], x['bottom_mid_y'], x['in_shadow']))\n",
        "    coordinates_and_shadow.sort(key=lambda t: t[0])\n",
        "\n",
        "    geocoordinates_and_shadow = list(zip(x['frame'], x['latitude'], x['longitude'], x['in_shadow']))\n",
        "    geocoordinates_and_shadow.sort(key=lambda t: t[0])\n",
        "\n",
        "    # Identify valid segments\n",
        "    valid_segments = []\n",
        "    current_segment = [coordinates_and_shadow[0]]\n",
        "\n",
        "    for i in range(1, len(coordinates_and_shadow)):\n",
        "        # Calculate the displacement between consecutive frames\n",
        "        displacement = np.sqrt((coordinates_and_shadow[i][1] - coordinates_and_shadow[i-1][1])**2 +\n",
        "                               (coordinates_and_shadow[i][2] - coordinates_and_shadow[i-1][2])**2)\n",
        "\n",
        "        frame_gap = coordinates_and_shadow[i][0] - coordinates_and_shadow[i-1][0]\n",
        "\n",
        "        if frame_gap <= 2 and displacement <= MAX_DISPLACEMENT:\n",
        "            current_segment.append(coordinates_and_shadow[i])\n",
        "        else:\n",
        "            if len(current_segment) >= MIN_PRESENCE_DURATION:\n",
        "                valid_segments.append(current_segment)\n",
        "            current_segment = [coordinates_and_shadow[i]]\n",
        "\n",
        "        if frame_gap > MAX_FRAME_GAP:\n",
        "            break\n",
        "\n",
        "    # Add the last segment if it is valid\n",
        "    if len(current_segment) >= MIN_PRESENCE_DURATION:\n",
        "        valid_segments.append(current_segment)\n",
        "\n",
        "    # Flatten valid segments into a single list\n",
        "    valid_coordinates_and_shadow = [item for segment in valid_segments for item in segment]\n",
        "\n",
        "    # Extract start frame, end frame, and in_shadow statuses\n",
        "    start_frame = valid_coordinates_and_shadow[0][0] if valid_coordinates_and_shadow else None\n",
        "    end_frame = valid_coordinates_and_shadow[-1][0] if valid_coordinates_and_shadow else None\n",
        "    in_shadow_statuses = [status for _, _, _, status in valid_coordinates_and_shadow]\n",
        "\n",
        "    # Calculate the number of frames they are present in\n",
        "    frames_present = len(valid_coordinates_and_shadow)\n",
        "    seconds_present = frames_present * 2 / 5  # Since we have 5fps and skip every other frame\n",
        "\n",
        "    return start_frame, end_frame, in_shadow_statuses, valid_coordinates_and_shadow, geocoordinates_and_shadow, frames_present, seconds_present\n",
        "\n",
        "# Group data by person_id and apply the process_group function\n",
        "grouped_data = data.groupby('person_id').apply(process_group).reset_index()\n",
        "\n",
        "# Split the result into separate columns\n",
        "grouped_data[['start_frame', 'end_frame', 'in_shadow_statuses', 'coordinates_and_shadow', 'geocoordinates_and_shadow', 'frames_present', 'seconds_present']] = pd.DataFrame(grouped_data[0].tolist(), index=grouped_data.index)\n",
        "grouped_data = grouped_data.drop(columns=[0])\n",
        "\n",
        "# Filter out tracks with no valid segments\n",
        "filtered_data = grouped_data[grouped_data['frames_present'] > 0]\n",
        "\n",
        "# Display the filtered data\n",
        "print(filtered_data)"
      ],
      "metadata": {
        "id": "gWsItbd3IKlD"
      },
      "id": "gWsItbd3IKlD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categorizing"
      ],
      "metadata": {
        "id": "pulrRpgEwdRU"
      },
      "id": "pulrRpgEwdRU"
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize_person(data, buffer_frames=10):\n",
        "    # Extract in_shadow status\n",
        "    in_shadow_status = [frame[3] for frame in data]\n",
        "\n",
        "    # Apply buffer\n",
        "    in_shadow_buffered = []\n",
        "    buffer_count = 0\n",
        "    for status in in_shadow_status:\n",
        "        if status:\n",
        "            buffer_count = buffer_frames  # Reset buffer\n",
        "        elif buffer_count > 0:\n",
        "            status = True\n",
        "            buffer_count -= 1\n",
        "        in_shadow_buffered.append(status)\n",
        "\n",
        "    # Calculate the percentage of buffered frames in the shade\n",
        "    shade_count = sum(in_shadow_buffered)\n",
        "    total_frames = len(in_shadow_buffered)\n",
        "    shade_percentage = shade_count / total_frames\n",
        "\n",
        "    start_in_shadow = in_shadow_buffered[0]\n",
        "    end_in_shadow = in_shadow_buffered[-1]\n",
        "\n",
        "    if start_in_shadow and end_in_shadow:\n",
        "        return 'photophobic'\n",
        "    elif not start_in_shadow and not end_in_shadow:\n",
        "        category = 'heliophile'\n",
        "    elif start_in_shadow and not end_in_shadow:\n",
        "        category = 'sun-chaser'\n",
        "    elif not start_in_shadow and end_in_shadow:\n",
        "        return 'shade-chaser'\n",
        "    else:\n",
        "        return 'unknown'\n",
        "\n",
        "    # If the initial category is heliophile or sun-chaser and more than 40% of frames are in the shade\n",
        "    if category in ['heliophile', 'sun-chaser'] and shade_percentage > 0.40:\n",
        "        return 'shade-chaser'\n",
        "\n",
        "    return category\n",
        "\n",
        "# Apply the categorization to the filtered data\n",
        "filtered_data['category'] = filtered_data['coordinates_and_shadow'].apply(categorize_person)"
      ],
      "metadata": {
        "id": "cozk2bipvSi2"
      },
      "id": "cozk2bipvSi2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_ratio = filtered_data['category'].value_counts(normalize=True)\n",
        "print(category_ratio)"
      ],
      "metadata": {
        "id": "0DVqs6sfvaXU"
      },
      "id": "0DVqs6sfvaXU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shade_data = pd.read_csv(shade_data_file)\n",
        "\n",
        "# Define the interval around start_frame and end_frame to calculate the average shadow_area\n",
        "interval = 100  # Adjust this value based on your needs\n",
        "\n",
        "# Function to calculate the average shadow_area over an interval\n",
        "def calculate_average_shade_area(frame, shade_data, interval):\n",
        "    start_interval = frame - interval\n",
        "    end_interval = frame + interval\n",
        "    interval_data = shade_data[(shade_data['frame'] >= start_interval) & (shade_data['frame'] <= end_interval)]\n",
        "    return interval_data['shadow_area'].mean()\n",
        "\n",
        "# Calculate the average shadow_area for start_frame and end_frame\n",
        "filtered_data['start_shade_area_avg'] = filtered_data['start_frame'].apply(lambda x: calculate_average_shade_area(x, shade_data, interval))\n",
        "filtered_data['end_shade_area_avg'] = filtered_data['end_frame'].apply(lambda x: calculate_average_shade_area(x, shade_data, interval))"
      ],
      "metadata": {
        "id": "GyK2i90qvf7s"
      },
      "id": "GyK2i90qvf7s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to a new CSV file\n",
        "final_data_file = os.path.join(video_output_dir, f\"{file_name}_final_data.csv\")\n",
        "filtered_data.to_csv(final_data_file, index=False)\n",
        "\n",
        "print(f\"FINAL CSV saved to {final_data_file}\")"
      ],
      "metadata": {
        "id": "ZnJ0ov3lIIDN"
      },
      "id": "ZnJ0ov3lIIDN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot paths with colors based on categories\n",
        "def plot_paths_with_categories(df):\n",
        "    category_colors = {\n",
        "        'photophobic': 'red',\n",
        "        'heliophile': 'green',\n",
        "        'sun-chaser': 'blue',\n",
        "        'shade-chaser': 'purple',\n",
        "        'unknown': 'gray'\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for index, row in df.iterrows():\n",
        "        path = row['coordinates_and_shadow']\n",
        "        category = row['category']\n",
        "        color = category_colors.get(category, 'black')  # Default to black if category is not found\n",
        "        x_coords, y_coords = zip(*[(coord[1], coord[2]) for coord in path])\n",
        "        plt.plot(x_coords, y_coords, linestyle='-', color=color, label=category if index == 0 else \"\")\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel('X Coordinate')\n",
        "    plt.ylabel('Y Coordinate')\n",
        "    plt.title('Paths Visualization with Categories')\n",
        "\n",
        "    # Add legend\n",
        "    handles, labels = plt.gca().get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels, handles))\n",
        "    plt.legend(by_label.values(), by_label.keys())\n",
        "\n",
        "    # Show the plot\n",
        "    plt.grid(True)\n",
        "    pdf.savefig()\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# Example usage\n",
        "plot_paths_with_categories(filtered_data)"
      ],
      "metadata": {
        "id": "Hbja4uhjsjqy"
      },
      "id": "Hbja4uhjsjqy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scatter Plot"
      ],
      "metadata": {
        "id": "fUp4a7EuRs2A"
      },
      "id": "fUp4a7EuRs2A"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install seaborn"
      ],
      "metadata": {
        "id": "hehV4rqzRuhZ"
      },
      "id": "hehV4rqzRuhZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load filtered detections to get the list of kept ids\n",
        "filtered_detections = pd.read_csv(final_data_file)\n",
        "\n",
        "# Get the list of kept ids\n",
        "kept_ids = filtered_detections['person_id'].unique()\n",
        "\n",
        "# Load the transformed data\n",
        "transformed_data = pd.read_csv(transformed_output_file)\n",
        "\n",
        "# Filter the transformed data to keep only the rows with the kept ids\n",
        "ungrouped_filtered_data = transformed_data[transformed_data['person_id'].isin(kept_ids)]\n",
        "\n",
        "# Save the filtered transformed data to a new CSV file\n",
        "ungrouped_output_file = os.path.join(video_output_dir, f\"{file_name}_ungrouped_filtered_data.csv\")\n",
        "ungrouped_filtered_data.to_csv(ungrouped_output_file, index=False)\n",
        "\n",
        "print(f\"Filtered transformed data saved to {ungrouped_output_file}\")"
      ],
      "metadata": {
        "id": "dRSUTJTnxxyP"
      },
      "id": "dRSUTJTnxxyP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Geocoordinates"
      ],
      "metadata": {
        "id": "i1AGu7leI6Yy"
      },
      "id": "i1AGu7leI6Yy"
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv(ungrouped_output_file)  # Ensure your file name is correct\n",
        "\n",
        "# Create a scatter plot of object positions\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "sns.scatterplot(\n",
        "    x='latitude',\n",
        "    y='longitude',\n",
        "    data=data,\n",
        "    s=50,  # Adjust the size of the dots\n",
        "    alpha=0.6,  # Adjust the transparency of the dots\n",
        "    color='blue',\n",
        "    ax=ax\n",
        ")\n",
        "ax.set_title('Object Positions Scatter Plot')\n",
        "ax.set_xlabel('Latitude')\n",
        "ax.set_ylabel('Longitude')\n",
        "\n",
        "# Save the current plot to the PDF\n",
        "pdf.savefig(fig)\n",
        "plt.show()\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "id": "Y21ygN8cI5Ec"
      },
      "id": "Y21ygN8cI5Ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_data = data.groupby('person_id')\n",
        "\n",
        "# Create a figure for the plot\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "for object_id, group in grouped_data:\n",
        "    ax.plot(group['latitude'], group['longitude'], label=f'Object {object_id}')\n",
        "\n",
        "ax.set_xlabel('Latitude')\n",
        "ax.set_ylabel('Longitude')\n",
        "ax.set_title('Object Paths')\n",
        "ax.legend()\n",
        "\n",
        "# Save the current plot to the PDF\n",
        "pdf.savefig(fig)\n",
        "plt.show()\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "id": "REwC5UOvJMCv"
      },
      "id": "REwC5UOvJMCv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf.close()\n",
        "\n",
        "print(f\"All plots have been saved to {pdf_file}\")"
      ],
      "metadata": {
        "id": "VY_szyV0SQOb"
      },
      "id": "VY_szyV0SQOb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}